---
title: "Exercise 1: Dataset split and CV"
output: html_notebook
---

CUSTOM FUNCTIONS FOR TASKS

```{r}
### this is a custom function to evaluate models using RMSE and explained variance R^2, you can just run it as is and use it later 
eval_metrics = function(pred, true){
  SSE = sum((pred - true)^2)
  SST = sum((true - mean(true))^2)
  R_square = 1 - SSE / SST
  RMSE = sqrt(SSE/length(pred))
  data.frame(
  RMSE = RMSE,
  Rsquare = R_square)
  }
```

LOAD PACKAGES 
```{r}
packs = c('tidyverse', 'readr', 'lme4', 'rsample', 'caTools', 'glmnet', 'caret', 'mltools')
if (!require("pacman")) install.packages("pacman")
pacman::p_load(packs, update=F, character.only = T)

```

DATA LOADING AND CODING
```{r}
### load dataset "Insurance_c.csv"
data_path <- "../data/"

data <- data.table::fread(paste0(data_path, "Insurance_c.csv"))

### define and recode variables
# 1. chr to num: age, bmi, charges 
# 2. chr to factor: sex, smoker, region. Recode NA as an additional factor level
data <- data %>% 
  mutate(
    sex = factor(sex, exclude = NULL),
    smoker = factor(smoker, exclude = NULL),
    region = factor(region, exclude = NULL)
  )


```

TASK 1: DATASET SPLIT AND OVERFITTING ESTIMATION

1. Step: DATA SPLITTING

```{r}
# set seed for reproducibility
set.seed(0)

# split dataset into train - test (ratio: 90%/10%) // Hint: use the split function from "caTools"
data <- data %>% 
  mutate(
    subject = V1
  ) %>% 
  mutate(
    is_training = caTools::sample.split(subject, 0.9)
  )
```

```{r}
# set seed for reproducibility
set.seed(0)

# split dataset into train - val - test (ratio: 80 - 10 - 10)

# Hint: you can use the "initial_validation_split" function from 'rsample' package
split_data <- rsample::initial_validation_split(
  data,
  prop = c(0.8, 0.1)
)
## Add code here ##
```

TASK 2: MODEL FITTING
Note: For the following task, use the train - test split (90/10) you created 

```{r}
### 1. Step: Fit an OLS regression model on the training set "train" with "charges" as outcome and all other variables as predictors

# fit model (lm)
## Add code here ##
training_data <- training(split_data)
test_data <- testing(split_data)
validation_data <- validation(split_data)

lm_training <- lm(charges ~ ., data = training_data[, -c("V1", "subject", "is_training")])

### 2. Step: Evaluate on test set "test", check for amount of overfitting using RMSE and explained variance r2 as metrics ##

# 2.1 predict raw values (outcome) for the train set and test set // Hint: use the "predict" function
## Add code here ##

predicted_charges <- predict(lm_training, test_data)

# 2.2. calculate and print r2 and rmse scores for train and test set // Hint: use the custom function from the beginning
## Add code here ##
eval_lm <- eval_metrics(predicted_charges, test_data$charges)
eval_metrics(predict(lm_training), training_data$charges)
```

```{r}
### Model 2: Ridge Regression; NOTE: You can ignore the code and everything else for now, just run this block ###

# prepare data: encode factors, separate predictors (matrix format) and outcome (vector)
train_ohe = one_hot(data.table::as.data.table(training_data[, -c("V1", "subject", "is_training")]))
test_ohe = one_hot(data.table::as.data.table(test_data[, -c("V1", "subject", "is_training")]))
x_train = model.matrix(charges~., train_ohe)[,-1]
x_test = model.matrix(charges~., test_ohe)[,-1]
y_train = train_ohe %>%
  select(charges) %>%
  unlist() %>%
  as.numeric()

# set seed
set.seed(0)
#find optimal lambda value that minimizes MSE on train set
nFolds = 10
foldid = sample(rep(seq(nFolds), length.out = nrow(training_data)))
cv_model = cv.glmnet(x = as.matrix(x = x_train), 
                   y = y_train, alpha = 0, 
                   nfolds = nFolds,
                   foldid = foldid)
best_lambda = cv_model$lambda.min

# fit best model
best_model = glmnet(x_train, y_train, alpha = 0, lambda = best_lambda)

### Evaluate model 2 ##
# predict raw values for the train set and test set
predictions2 = predict(best_model, s=best_lambda, newx=x_train)
predictions_test2 = predict(best_model, s = best_lambda, newx = x_test)

# # calculate and print r2 and rmse scores for train and test set
score_train1 = eval_metrics(predictions2, train_ohe$charges)
print(c("Score Train Model 2: RIDGE Regression", score_train1))

score_test1 = eval_metrics(predictions_test2, test_ohe$charges)
print(c("Score Test Model 2: RIDGE Regression", score_test1))
```


What do you observe regarding the model performance? Which of the two models would you prefer when it comes to predicting "charges" and why? 

### Space to write your own observations/thoughts ###
First model has better prediction in the testing dataset


# -------------------------------------------------------------- #

TASK 2: CROSS VALIDATION

```{r}
# Do cross validation on the train dataset, using 10 folds with the standard OLS model formulation from the previous task
# Hint: To make things easier, you can use the trainControl function from the "caret" package 
set.seed(0)
## Add code here ##
train.control <- caret::trainControl(method = "cv", number = 10)

# Train the model
# Hint: Use the "train" function from the "caret" package, method = "lm"
## Add code here ##
cv_model <- train(
  charges ~ .,             # Formula for the model
  data = training_data[, -c("V1", "subject", "is_training")],             # Data to be used
  method = "lm",      
  trControl = train.control # Use the defined training control
)


# Print the results (mean and SD of RMSE, Rsquared over all folds)
## Add code here ##
print(cv_model, showSD = TRUE)
```

```{r}
# Do the same for Ridge Regression model // Just run this block
# Train a ridge regression model
set.seed(0)
model2 = train(x_train, train_ohe$charges, method = 'glmnet', 
               tuneGrid = expand.grid(alpha = 0, lambda = best_lambda), trControl = train.control)

print(model2, showSD = T)
```

What do you observe regarding both model's bias and variance? Which model would you prefer and why?
### Space to write down your own thoughts ###


### END OF TASKS ###
# -------------------------------------------------------------- #


