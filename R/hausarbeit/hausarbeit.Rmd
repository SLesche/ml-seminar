---
title: "Hausarbeit ML-Seminar"
output: html_notebook
---

```{r setup, eval = TRUE, warning = FALSE, include = TRUE}
# Load packages
library(tidyverse)
library(baseballr)
library(glmnet)
library(randomForest)
library(e1071) 
library(caTools) 
library(class) 

source("helper_functions.R")
source("ml_functions.R")

knitr::opts_chunk$set(eval = FALSE)
knitr::opts_chunk$set(warning = FALSE)

```

# Bang - Bang: Who Can Beat the Astros?
I love baseball! And I am not they only one... from the crack of the bat to the roar of the crowd, it has captivated fans for over a century. At its heart, baseball is a game of strategy, where pitchers and batters engage in a delicate dance of skill and anticipation. One team that has recently been at the center of this dance, both for their prowess and controversy, is the Houston Astros.

The Astros have been a dominant force in Major League Baseball (MLB) in recent years, known for their talented roster and innovative approaches to the game. However, their success has been marred by a major scandal that shook the sports world. In 2017, the Astros were found guilty of using an illicit sign-stealing scheme during their championship-winning season. This involved using technology to decode the signs of opposing players and relaying this information to their batters in real-time, giving them an unfair advantage by allowing them to predict the incoming pitch in advance. They then relayed that information to their players by banging on a trashcan once, if the incoming pitch was a slower pitch, and not doing anything if the incoming pitch was a fastball (hence the title).

So I thought, let's try to do just that: predict whether the next pitch was a fastball or not. However, I want to do this fairly, by using publicly available data and preferably not having to bang on any trashcans. By analyzing vast amounts of data, machine learning models can provide insights into pitching patterns and tendencies, potentially leveling the playing field. In this work, I'll explore how machine learning can be used to predict the next pitch, giving teams a new tool in their arsenal to outsmart even the most cunning opponents. From the basic principles of machine learning to its application in baseball, we'll dive deep into the technology that could hold the key to beating the Astros and others like them.

## Baseball
Before we dive deep into predicting the future of baseball, I want to provide a quick introduction to the sport. Baseball is a bat-and-ball sport played between two teams of nine players each, taking turns batting and fielding. The game is played on a diamond-shaped field, which includes four bases arranged in a square around the pitcher's mound.

### The Pitcher
In baseball, the pitcher is one of the most crucial players on the field. Positioned on the pitcher's mound, a slight hill in the middle of the field, the pitcher throws the ball towards the catcher, who is behind home plate. The pitcher's primary goal is to get the batter out by using a variety of pitches to make it difficult for the batter to hit the ball.

Pitchers use these different types of pitches to deceive batters and gain an advantage. Here are some common pitch types:

- **Fastball**: A high-speed pitch, typically thrown with little to no movement. It's the most basic pitch and can be further categorized into four-seam, two-seam, and cut fastballs.
- **Curveball**: A pitch with significant downward movement, often causing the ball to "break" sharply as it approaches the plate.
- **Slider**: A pitch that combines speed with a lateral, diagonal break. It is faster than a curveball but with less dramatic movement.
- **Changeup**: A slower pitch meant to look like a fastball, intended to disrupt the batter's timing.
- **Splitter**: A pitch that appears like a fastball but drops sharply as it nears the plate.

### The Batter
The offensive player, the batter, will try to hit those different types of pitches. After a hit, the game transforms to something similar to "Brennball" where the ball travels and you can run between safe spots ("bases") before the ball reaches you again. The exact scoring format is not that important. However, knowing which pitch type is coming would severly benefit a batters ability to perform.

## Data
In the world of modern baseball analytics, data is king. To predict the next pitch accurately, we need access to detailed and comprehensive datasets that capture every nuance of the game. This is where the `baseballr` package comes in handy. The `baseballr` package allows us to access and manipulate baseball data from various sources, including the MLB's Statcast system.

The `baseballr` package streamlines the process of retrieving game data, making it accessible for analysis. With some work (shown in the `get_pbp_data()` function of `helper_functions.R`), we can pull detailed pitch-by-pitch data, including information on pitch type, velocity, spin rate, and much more. 

As an example, I loaded the pitch statistics of 6 different pitchers. I then visualized what types of pitches they throw, when they throw them, what percentage of the time they throw each pitch and how fast each pitch is thrown.

```{r data}
# Getting data ----
id_map <- data.table::fread("./data/player_id_map.csv") %>%
  rename("bbref_id" = BREFID)

# scherzer_data <- get_pbp_data("Max Scherzer", id_map)
# data.table::fwrite(scherzer_data, "./data/scherzer_data.csv")
scherzer_data <- data.table::fread("./data/scherzer_data.csv")

# hader_data <- get_pbp_data("Josh Hader", id_map)
# data.table::fwrite(hader_data, "./data/hader_data.csv")
hader_data <- data.table::fread("./data/hader_data.csv")

# cole_data <- get_pbp_data("Gerrit Cole", id_map)
# data.table::fwrite(cole_data, "./data/cole_data.csv")
cole_data <- data.table::fread("./data/cole_data.csv")

# darvish_data <- get_pbp_data("Yu Darvish", id_map)
# data.table::fwrite(darvish_data, "./data/darvish_data.csv")
darvish_data <- data.table::fread("./data/darvish_data.csv")

# verlander_data <- get_pbp_data("Justin Verlander", id_map)
# data.table::fwrite(verlander_data, "./data/verlander_data.csv")
verlander_data <- data.table::fread("./data/verlander_data.csv")

# kimbrel_data <- get_pbp_data("Craig Kimbrel", id_map)
# data.table::fwrite(kimbrel_data, "./data/kimbrel_data.csv")
kimbrel_data <- data.table::fread("./data/kimbrel_data.csv")
```

```{r cleaning}
clean_data <- scherzer_data %>% clean_pbp_data()
```

## Visualizing pitches
Here we see the pitch usage by Max Scherzer. In the end, this is the data I will demonstrate all the machine learning models on. "Mad" Max Scherzer is an ideal subject for our analysis for several reasons. Firstly, Scherzer is widely regarded as one of the best pitchers in baseball. His career, marked by multiple Cy Young Awards and All-Star appearances, speaks volumes about his skill and consistency.

More importantly, Scherzer has maintained a remarkably stable pitch mix since 2017. This consistency makes him an excellent candidate for predictive modeling, as it allows us to identify patterns and trends more reliably. A pitcher with a stable repertoire provides a clearer signal for our machine learning algorithms, improving the accuracy of our predictions.

By focusing on Scherzer, we leverage the wealth of data available from his extensive and consistent career to build a robust model. This not only enhances our understanding of his pitching strategies but also serves as a blueprint for applying similar techniques to other pitchers and teams.

```{r pitch-usage}
plot_pitch_usage(clean_data)
```
In the following plots, you can see how different pitches move and why they might be classified into different categories. You can also see differences in velocity that give rise to the description "fastball". Both the "Cutter" and the "4-Seam Fastball" would fall into this category, being thrown over 140km/h. 
```{r pitch-stats}
plot_pitch_movement(clean_data %>% filter(game_year > 2018))

plot_pitch_velocity(clean_data %>% filter(game_year > 2018))

plot_pitch_by_count(clean_data %>% filter(game_year > 2018))

plot_pitch_by_batter(clean_data %>% filter(game_year > 2018))
```
# Machine Learning
In the following sections, we'll delve into the machine learning methodologies used to predict pitch types and how this data-driven approach can potentially level the playing field against even the most formidable opponents, like the Astros.

Firstly, we have to prepare the data to fit our machine learning functions and engineer some additional features that might help us predict future pitches. We will focus on all the pitches Max has thrown after 2017. The `prep_for_ml` function performs the following feature engineering tasks on the baseball data to prepare it for machine learning:

### 1. **Calculating Intended Target Position**
   - Computes the mean position (`intended_x`, `intended_z`) of the pitches for each combination of `game_year`, `stand`, `pitch_type`, `balls`, and `strikes`.

### 2. **Pitch Mix Calculation**
   - Calculates the frequency of each pitch type for each combination of `game_year`, `stand`, `balls`, and `strikes`.
   - The frequencies are then pivoted to wide format with each pitch type frequency becoming a separate column (`freq_*`).

### 3. **Data Cleaning and Joining**
   - Replaces empty string values with `NA`.
   - Joins the calculated intended target positions and pitch mix frequencies back to the original dataset.

### 4. **Adding New Variables**
   - **Pitch Count**: Adds a running count of pitches (`pitch_count_total`) for each game.
   - **Men on Base**: Creates a `men_on_base` variable by summing indicators for base runners.
   - **Runners in Scoring Position**: Creates a binary variable `runners_in_scoring_position` to indicate if there are runners on 2nd or 3rd base.
   - **Score Difference**: Computes the score difference (`fld_score` - `bat_score`).
   - **Dodged a Hit**: Indicates if the estimated batting average (`estimated_ba_using_speedangle`) is greater than 0.5.
   - **Distance from Intended**: Calculates the Euclidean distance from the intended target position.
   - **Opponent Team**: Determines the opponent team based on the inning (`inning_topbot`).
   - **Sweet Spot**: Indicates if the launch angle is between 8 and 32 degrees.
   - **Hard Hit**: Indicates if the launch speed is greater than 95 mph.
   - **Is Hit**: Indicates if the event was a hit (e.g., single, double).
   - **Is Base**: Indicates if the event resulted in the batter reaching base.
   - **Is Fastball**: Indicates if the pitch type is a fastball (e.g., `FC`, `FF`, `SI`).

### 5. **Additional Flags**
   - **Yanked**: Indicates if the distance from the intended target is greater than 7.
   - **Barrel**: Indicates if both `sweet_spot` and `hard_hit` are true.

### 6. **Rolling Averages for Fastballs**
   - Computes rolling averages of the `is_fastball` variable over windows of 5, 10, 20 pitches, and the entire game (`rolling_fastball_avg_*`).

### 7. **Converting Categorical Variables**
   - Converts specific variables to factors and handles `NA` values by treating them as a separate category.
   - Uses `fct_lump_min` to lump together infrequent factor levels.

### 8. **Lagging Variables**
   - Creates lagged versions of certain variables (`lag1_`, `lag2_`, etc.) for the past 5 pitches within each game and inning.

### 9. **Final Selection**
   - Selects relevant and lagged variables for the final dataset.

The final dataset returned by `prep_for_ml` is thus enriched with engineered features, cleaned, and structured for machine learning applications. 

```{r ml-prep}
# Recoding Data ----
ml_data <- clean_data %>% filter(game_year > 2017) %>%  prep_for_ml()

outcome <- ml_data %>% select(pitch_type, is_fastball)

ml_ohe <- mltools::one_hot(
  data.table::as.data.table(ml_data %>% select(-pitch_type, -is_fastball))
  ) %>% 
  janitor::remove_constant() %>% 
  janitor::remove_empty()

ml_ohe$outcome <- factor(outcome$is_fastball)

split_data <- rsample::initial_validation_split(ml_ohe, c(0.9, 0.05))

training_data <- rsample::training(split_data) %>% 
  select_vars_for_ml(2)

testing_data <- rsample::testing(split_data) %>% 
  select_vars_for_ml(2)

validation_data <- rsample::validation(split_data) %>% 
  select_vars_for_ml(2)
```

## Models
We will focus on predicting which overall type of pitch is coming: "Fastball or not?". I did this to reduce complexity of the models. You can also try changing the `outcome` variable to `pitch_type` which would change the models (except the Regression models) towards trying to classify exact pitch types.

Predicting fastballs or off-speed pitches is also more historically accurate (looking at you... Astros).


### The null model
```{r take-the-best}
take_the_best <- ml_data %>% 
    group_by(game_year, stand, balls, strikes) %>%
    count(is_fastball) %>% 
    mutate(
      freq = n / sum(n)
    ) %>% 
    # filter(pitch_type != "") %>% 
  slice_max(freq) %>% 
  mutate(
    mult = n * freq
  ) %>% 
  ungroup() %>% 
  summarize(
    accuracy = sum(mult) / sum(n)
  )

# Here accuracy when taking the best just based on balls/strikes/
```
This is a model that simply predicts the most common type of pitch thrown in a specific scenario everytime. For Max Scherzer, this is a fastball in all scenarios. The accuracy of this model is `r take_the_best$accuracy`.

Now, lets see if ML models can outperform this:
For all models, we used the train-validation-test split to hypertune some parameter for optimal accuracy in the validation set. Then we computed the accuracy in the test set as a final indicator. You can find more info in the source code of each function (located in the `ml_functions.R` script).

### Lasso
```{r lasso}
lasso_result <- run_lasso_regression(training_data, testing_data, validation_data)
```
The accuracy in the testing data is `r lasso_result$test_acc`.

### Ridge
```{r ridge}
ridge_result <- run_ridge_regression(training_data, testing_data, validation_data)
```
The accuracy in the testing data is `r ridge_result$test_acc`.

### Random Forest
```{r random-forest}
rf_result <- run_random_forest(training_data, testing_data, validation_data)
```
The accuracy in the testing data is `r rf_result$test_acc`.

Random Forest Models make visualizing the importance of features quite simple. I chose two ways to visualize the importance of each feature, SHAP values and permutation-based importance.

SHAP values provide a unified measure of feature importance based on cooperative game theory, specifically Shapley values. Here’s how SHAP importance works:

1. **Shapley Values**: Shapley values come from cooperative game theory and provide a fair way to distribute the "payout" among players (features in this context). They consider all possible combinations of features and how the inclusion of a specific feature changes the prediction.
  
2. **Model Explanation**: SHAP values explain the prediction of an instance by computing the contribution of each feature to the prediction. For Random Forests, SHAP values represent the average marginal contribution of a feature across all possible subsets of features.
  
3. **Global Feature Importance**: By averaging the absolute SHAP values of each feature across all instances in the dataset, you get a global measure of feature importance. This tells you which features have the most impact on the model's predictions overall.

Permutation importance assesses the importance of a feature by measuring the change in the model's performance when the values of that feature are randomly shuffled. Here's how it works:

1. **Baseline Performance**: First, compute the performance of the model using an appropriate metric (e.g., accuracy for classification, mean squared error for regression) on a validation set or the out-of-bag samples in the case of Random Forests.

2. **Feature Permutation**: For each feature, permute (randomly shuffle) its values across all instances in the validation set. This breaks the relationship between the feature and the target variable while keeping the values of other features intact.

3. **Measure Impact**: Recompute the model performance on the dataset with the permuted feature. The decrease in performance (compared to the baseline) indicates the importance of the feature. The larger the drop in performance, the more important the feature is.

```{r rf-viz}
rf_imp <- get_predictor_importance_rf(rf_result$model, training_data)

plot(rf_imp$importance) +
  ggtitle("PFI") +
  xlab('Reduction in MSE (compared to average loss) when predictor included')

# Plotting
shap_values = shapviz::shapviz(rf_imp$shap)
shapviz::sv_importance(shap_values, show_numbers = T)
shapviz::sv_importance(shap_values, kind='bee')
treeshap::plot_contribution(rf_imp$shap, obs= 4)+
  ggtitle('SHAP Break-down: Sample no.5')+
  ylab('SHAP value')
```

### K-Nearest-Neighbor
```{r knn}
knn_result <- run_knn(training_data, testing_data, validation_data)
```
The accuracy in the testing data is `r knn_result$test_acc`.


### Support Vector Machines
```{r svm}
svm_result <- run_svm(training_data, testing_data, validation_data)
```
The accuracy in the testing data is `r svm_result$test_acc`.

```{r svm-linear}
svm_linear_result <- run_svm_linear(training_data, testing_data, validation_data)
```
The accuracy in the testing data is `r svm_linear_result$test_acc`.


## The winner is...
Here, show best models

# Conclusion
We did x good... is that good enough to beat the Astros? Time will tell. It was interesting that MODEL X won and that PREDICTOR Y was of maximum importance
