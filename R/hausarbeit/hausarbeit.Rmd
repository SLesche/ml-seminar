---
title: "Hausarbeit ML-Seminar"
output: html_notebook
---

```{r setup}
# Load packages
library(tidyverse)
library(baseballr)
library(glmnet)
library(randomForest)
library(e1071) 
library(caTools) 
library(class) 

source("helper_functions.R")
source("ml_functions.R")
```

# Bang - Bang: Who Can Beat the Astros?
Baseball, America's beloved pastime, is a sport rich in tradition and history. From the crack of the bat to the roar of the crowd, it has captivated fans for over a century. At its heart, baseball is a game of strategy, where pitchers and batters engage in a delicate dance of skill and anticipation. One team that has recently been at the center of this dance, both for their prowess and controversy, is the Houston Astros.

The Astros have been a dominant force in Major League Baseball (MLB) in recent years, known for their talented roster and innovative approaches to the game. However, their success has been marred by a major scandal that shook the sports world. In 2017, the Astros were found guilty of using an illicit sign-stealing scheme during their championship-winning season. This involved using technology to decode the signs of opposing players and relaying this information to their batters in real-time, giving them an unfair advantage by allowing them to predict the incoming pitch in advance.

The scandal, often referred to as the "Bang-Bang" scandal, after the sounds allegedly used to signal the pitches to the batters, resulted in severe repercussions. Several team members and staff were fined, suspended, and publicly scrutinized. Despite this, the Astros have continued to be a formidable team, raising the question: who can beat the Astros?

In the wake of the scandal, teams across the league have sought ways to counter the Astros' strategies and regain a competitive edge. This brings us to the intersection of baseball and cutting-edge technology: the use of machine learning to predict the next pitch. By analyzing vast amounts of data, machine learning models can provide insights into pitching patterns and tendencies, potentially leveling the playing field.In this blog post, we'll explore how machine learning is being used to predict the next pitch, giving teams a new tool in their arsenal to outsmart even the most cunning opponents. From the basic principles of machine learning to its application in baseball, we'll dive deep into the technology that could hold the key to beating the Astros and others like them.

# Data
Talk about how it is being read in

and which pitcher we are using
```{r data}
# Getting data ----
id_map <- data.table::fread("./data/player_id_map.csv") %>%
  rename("bbref_id" = BREFID)

# scherzer_data <- get_pbp_data("Max Scherzer", id_map)
# data.table::fwrite(scherzer_data, "./data/scherzer_data.csv")
scherzer_data <- data.table::fread("./data/scherzer_data.csv")

# hader_data <- get_pbp_data("Josh Hader", id_map)
# data.table::fwrite(hader_data, "./data/hader_data.csv")
hader_data <- data.table::fread("./data/hader_data.csv")

# cole_data <- get_pbp_data("Gerrit Cole", id_map)
# data.table::fwrite(cole_data, "./data/cole_data.csv")
cole_data <- data.table::fread("./data/cole_data.csv")

# darvish_data <- get_pbp_data("Yu Darvish", id_map)
# data.table::fwrite(darvish_data, "./data/darvish_data.csv")
darvish_data <- data.table::fread("./data/darvish_data.csv")

# verlander_data <- get_pbp_data("Justin Verlander", id_map)
# data.table::fwrite(verlander_data, "./data/verlander_data.csv")
verlander_data <- data.table::fread("./data/verlander_data.csv")

# kimbrel_data <- get_pbp_data("Craig Kimbrel", id_map)
# data.table::fwrite(kimbrel_data, "./data/kimbrel_data.csv")
kimbrel_data <- data.table::fread("./data/kimbrel_data.csv")
```

## How does the data cleaning work?
```{r cleaning}
clean_data <- scherzer_data %>% clean_pbp_data()
```

# Pitches
What are pitches, what can we see from pitch usage chart
```{r pitch-usage}
plot_pitch_usage(clean_data)
```

 Other pitch stats
```{r pitch-stats}
plot_pitch_movement(clean_data %>% filter(game_year > 2018))

plot_pitch_by_count(clean_data %>% filter(game_year > 2018))

plot_pitch_by_batter(clean_data %>% filter(game_year > 2018))

plot_pitch_velocity(clean_data %>% filter(game_year > 2018))
```
# Machine Learning
Describe concept and feature engineering done here
```{r ml-prep}
# Recoding Data ----
ml_data <- clean_data %>% filter(game_year > 2017) %>%  prep_for_ml()

outcome <- ml_data %>% select(pitch_type, is_fastball)

ml_ohe <- mltools::one_hot(
  data.table::as.data.table(ml_data %>% select(-pitch_type, -is_fastball))
  ) %>% 
  janitor::remove_constant() %>% 
  janitor::remove_empty()

ml_ohe$outcome <- factor(outcome$is_fastball)

split_data <- rsample::initial_validation_split(ml_ohe, c(0.9, 0.05))

training_data <- rsample::training(split_data) %>% 
  select_vars_for_ml(2)

testing_data <- rsample::testing(split_data) %>% 
  select_vars_for_ml(2)

validation_data <- rsample::validation(split_data) %>% 
  select_vars_for_ml(2)
```

## Models
```{r take-the-best}
take_the_best <-  ml_data %>% 
    group_by(game_year, stand, balls, strikes) %>%
    count(is_fastball) %>% 
    mutate(
      freq = n / sum(n)
    ) %>% 
    # filter(pitch_type != "") %>% 
  slice_max(freq) %>% 
  mutate(
    mult = n * freq
  ) %>% 
  ungroup() %>% 
  summarize(
    accuracy = sum(mult) / sum(n)
  )

# Here accuracy when taking the best just based on balls/strikes/
```

### Lasso
```{r lasso}
lasso_result <- run_lasso_regression(training_data, testing_data, validation_data)
```

### Ridge
```{r ridge}
ridge_result <- run_ridge_regression(training_data, testing_data, validation_data)
```

### Random Forest
```{r random-forest}
rf_result <- run_random_forest(training_data, testing_data, validation_data)
```

```{r rf-viz}
rf_imp <- get_predictor_importance_rf(rf_result$model, training_data)

plot(rf_imp$importance) +
  ggtitle("PFI") +
  xlab('Reduction in MSE (compared to average loss) when predictor included')

# Plotting
shap_values = shapviz::shapviz(rf_imp$shap)
shapviz::sv_importance(shap_values, show_numbers = T)
shapviz::sv_importance(shap_values, kind='bee')
treeshap::plot_contribution(rf_imp$shap, obs= 4)+
  ggtitle('SHAP Break-down: Sample no.5')+
  ylab('SHAP value')
```

### K-Nearest-Neighbor
```{r knn}
knn_result <- run_knn(training_data, testing_data, validation_data)
```

### Support Vector Machines
```{r svm}
svm_result <- run_svm(training_data, testing_data, validation_data)
```

## The winner is...


# Conclusion

